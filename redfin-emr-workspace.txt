from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Download data 
%%bash
wget -O - https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz | aws s3 cp - s3://store-raw-data-yml/city_market_tracker.tsv000.gz
--2023-10-23 18:38:14--  https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

# Create spark session
spark = SparkSession.builder.appName("RedfinDataAnalysis").getOrCreate()

# Read data from CSV
redfin_data = spark.read.csv("s3://store-raw-data-yml/city_market_tracker.tsv000.gz", header=True, inferSchema=True, sep= "\t")

# Show schema, columns, table
redfin_data.show(3)
redfin_data.printSchema()
redfin_data.columns

# Select data by columns
df_redfin = redfin_data.select(['period_end','period_duration', 'city', 'state', 'property_type', 'median_sale_price', 'median_ppsf', 'homes_sold', 'inventory', 'month_of_supply', 'median_dom', 'sold_above_list', 'last_updated'])
df_redfin.show(3)
print(f"Total number of rows: {df_redfin.count()}")

# Import functions check null values
from pyspark.sql.functions import isnull

# Check count null value in table
null_counts = [df_redfin.where(isnull(col_name)).count() for col_name in df_redfin.columns]
null_counts
for i, col_name in enumerate(df_redfin.columns):
  print(f"{col_name}: {null_counts[i]} null values)
remaining_count = df_redfin.na.drop().count()
print(f"Number of missing rows: {df_redfin.count() - remaining_count}")
print(f"Total number of remaining rows: {remaining_count}")

# Remove NA row value
df_redfin = df_redfin.na.drop()
print(f"Total number of rows: {df_redfin.count()}")

# Check count Null value in each column
null_counts = [df_redfin.where(isnull(col_name)).count() for col_name in df_redfin.columns]
null_counts

# Import functions year, month
from pyspark.sql.functions import year, month

# Extract year from period_end then save in a new column "period_end_year"
df_redfin = df_redfin.withColumn("period_end_year", year(col("period_end")))

# Extract month from period_end then save in a new column "period_end_month"
df_redfin = df_redfin.withColumn("period_end_month", month(col("period_end")))

# Drop period_end and last_updated columns
df_redfin = df_redfin.drop("period_end", "last_updated")

# Show 3 row in table
df_redfin.show(3)

# Import function When
from pyspark.sql.functions import when

# Update month number to month name in column "period_end_month"
df_redfin = df_redfin.withColumn("period_end_month",
                              when(col("period_end_month") == 1, "January")
                              .when(col("period_end_month") == 2, "February")
                              .when(col("period_end_month") == 3, "March")
                              .when(col("period_end_month") == 4, "April")
                              .when(col("period_end_month") == 5, "May")
                              .when(col("period_end_month") == 6, "June")
                              .when(col("period_end_month") == 7, "July")
                              .when(col("period_end_month") == 8, "August")
                              .when(col("period_end_month") == 9, "September")
                              .when(col("period_end_month") == 10, "October")
                              .when(col("period_end_month") == 11, "November")
                              .when(col("period_end_month") == 12, "December")
                              .otherwise("Unknown")
                            )
df.redfin.show(3)

# Write data frame to s3 bucket file parquet
s3_bucket = "s3://redfin-transform-zone-yml/redfin_data.parquet"
df_redfin.write.mode("overwrite).parquet(s3_bucket)









